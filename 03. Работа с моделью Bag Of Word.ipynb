{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование модели \"Мешка слов\" для обработки текстов на естественном языке\n",
    "Создадим рекомендательную систему, которая действует как вертикальная поисковая система. Система позволяет искать тексты из ограниченного количества документов, наиболее тесно связаны с заданной темой. Чтобы использовать возможности NLP, мы объединим методологию поиска с методами оценки семантического сходства.\n",
    "Загрузим файл .csv и во фрейм данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_15624\\4103388425.py:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATAPATH)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            sentence  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv \n",
    "DATAPATH = 'Data/movies_metadata.csv'\n",
    "df = pd.read_csv(DATAPATH)\n",
    "# Remove null description\n",
    "df = df[~df.isna()]\n",
    "# Renaming the description column\n",
    "df.rename(columns={'overview':'sentence'}, inplace=True)\n",
    "# Sampling the first 5000 rows\n",
    "df = df.iloc[:5000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем слишком короткие и слишком длинные предложения из нашего набора данных путем фильтрации с помощью MIN_WORDS и MAX_WORDS. \n",
    "Удаляем общеупотребительные слова (СТОП-СЛОВА), которые не помогают извлечь специфику данного предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning sentences...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "\n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace \n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Series of cleaning. String to lower case, remove non words characters and numbers (punctuation, curly brackets etc).\n",
    "        text (str): input text\n",
    "    return (str): modified initial text\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # lowercase text\n",
    "    # replace the matched string with ' '\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    Args:\n",
    "      sentence (str)\n",
    "      min_words (int)\n",
    "      max_words (int)\n",
    "      stopwords (set of string)\n",
    "      lemmatize (boolean)\n",
    "    returns:\n",
    "      list of string\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    return tokens    \n",
    "\n",
    "def clean_sentences(df):\n",
    "    \"\"\"\n",
    "    Remove irrelavant characters (in new column clean_sentence).\n",
    "    Lemmatize, tokenize words into list of words (in new column tok_lem_sentence).\n",
    "    Args: \n",
    "      df (dataframe)\n",
    "     returns:\n",
    "      df\n",
    "    \"\"\"\n",
    "    print('Cleaning sentences...')\n",
    "    df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
    "    df['tok_lem_sentence'] = df['clean_sentence'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS))\n",
    "    return df\n",
    "    \n",
    "df = clean_sentences(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая будет ранжировать лучшие рекомендации с учетом косинусного расстояния между векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def extract_best_indices(m, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens ans return best mathes.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # from highest idx to smallest score \n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение TF-IDF\n",
    "Как только мы рассчитаем статистику модели Tf-IDF (веса слов с учетом их частоты в тексте, нормированной на частоту во всем корпусе документов), мы сможем сгенерировать вектор встраивания 22 180 измерений для каждого описания фильма. \n",
    "Эти функции хранятся в матрице функций tfidf_mat, где каждая строка представляет собой запись описания фильма, встроенную в вектор функций.\n",
    "Когда мы получим запрос из пользовательского ввода, мы встроим его в одно и то же векторное пространство и сравним один за другим функцию предложения запроса embed_query с векторами предложений матрицы внедрения tfidf_mat ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>Basic Instinct</td>\n",
       "      <td>[{'id': 53, 'name': 'Thriller'}, {'id': 9648, ...</td>\n",
       "      <td>A police detective is in charge of the investi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Power 98</td>\n",
       "      <td>[{'id': 28, 'name': 'Action'}, {'id': 9648, 'n...</td>\n",
       "      <td>A controversial talk show host becomes involve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Mulholland Falls</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 9648, 'na...</td>\n",
       "      <td>In 1950s Los Angeles, a special crime squad of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        original_title                                             genres  \\\n",
       "1060    Basic Instinct  [{'id': 53, 'name': 'Thriller'}, {'id': 9648, ...   \n",
       "793           Power 98  [{'id': 28, 'name': 'Action'}, {'id': 9648, 'n...   \n",
       "693   Mulholland Falls  [{'id': 18, 'name': 'Drama'}, {'id': 9648, 'na...   \n",
       "\n",
       "                                               sentence  \n",
       "1060  A police detective is in charge of the investi...  \n",
       "793   A controversial talk show host becomes involve...  \n",
       "693   In 1950s Los Angeles, a special crime squad of...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_recommendations_tfidf(sentence, tfidf_mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the database sentences in order of highest cosine similarity relatively to each \n",
    "    token of the target sentence. \n",
    "    \"\"\"\n",
    "    # Embed the query sentence\n",
    "    tokens_query = [str(tok) for tok in tokenizer(sentence)]\n",
    "    embed_query = vectorizer.transform(tokens_query)\n",
    "    # Create list with similarity between query and dataset\n",
    "    mat = cosine_similarity(embed_query, tfidf_mat)\n",
    "    # Best cosine distance for each token independantly\n",
    "    best_index = extract_best_indices(mat, topk=3)\n",
    "    return best_index\n",
    "\n",
    "# Adapt stop words\n",
    "token_stop = tokenizer(' '.join(stopwords.words('english')), lemmatize=False)\n",
    "\n",
    "# Fit TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer) \n",
    "tfidf_mat = vectorizer.fit_transform(df['sentence'].values) # -> (num_sentences, num_vocabulary)\n",
    "\n",
    "# Return best threee matches between query and dataset\n",
    "test_sentence = 'a crime story with a beautiful woman' \n",
    "best_index = get_recommendations_tfidf(test_sentence, tfidf_mat)\n",
    "\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая находит лучший индекс из матрицы расстояний, берет среднее значение косинусного расстояния для каждого встроенного предложения и ранжирует результаты. Пока не обращайте внимания на аргумент маскировки, мы будем использовать его для другой модели.\n",
    "Эта модель очень проста в использовании и может быть настроена с помощью всего нескольких строк кода, обучение также очень быстрое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение word2vec\n",
    "Мы используем библиотеку gensim для обучения модели word2vec на наших данных корпуса. Сначала нам нужно предоставить словарь (список слов, которые мы хотим векторизовать), а затем обучить модель на нескольких эпохах. По умолчанию модель обучает CBOW.\n",
    "Основным недостатком Word2Vec является то, что он не может создавать векторы для слов, которых изначально не было в используемом для обучения словаре. Вот почему нам нужно предоставить функцию is_word_in_model , которая удаляет слова из предложения запроса, которые не видны в обучающем наборе.\n",
    "Мы используем встроенную функцию n_similarity для эффективного вычисления расстояния между запросом и предложениями набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Heavy Metal</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 878, ...</td>\n",
       "      <td>A glowing orb terrorizes a young girl with a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Homage</td>\n",
       "      <td>[{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...</td>\n",
       "      <td>The young caretaker at the estate of a reclusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Andre</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 10751, 'n...</td>\n",
       "      <td>The true story of how a marine seal named Andr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original_title                                             genres  \\\n",
       "602    Heavy Metal  [{'id': 16, 'name': 'Animation'}, {'id': 878, ...   \n",
       "395         Homage  [{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...   \n",
       "571          Andre  [{'id': 18, 'name': 'Drama'}, {'id': 10751, 'n...   \n",
       "\n",
       "                                              sentence  \n",
       "602  A glowing orb terrorizes a young girl with a c...  \n",
       "395  The young caretaker at the estate of a reclusi...  \n",
       "571  The true story of how a marine seal named Andr...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def is_word_in_model(word, model):\n",
    "    \"\"\"\n",
    "    Check on individual words ``word`` that it exists in ``model``.\n",
    "    \"\"\"\n",
    "    assert type(model).__name__ == 'KeyedVectors'\n",
    "    is_in_vocab = word in model.key_to_index.keys()\n",
    "    return is_in_vocab\n",
    "\n",
    "def predict_w2v(query_sentence, dataset, model, topk=3):\n",
    "    query_sentence = query_sentence.split()\n",
    "    in_vocab_list, best_index = [], [0]*topk\n",
    "    for w in query_sentence:\n",
    "        # remove unseen words from query sentence\n",
    "        if is_word_in_model(w, model.wv):\n",
    "            in_vocab_list.append(w)\n",
    "    # Retrieve the similarity between two words as a distance\n",
    "    if len(in_vocab_list) > 0:\n",
    "        sim_mat = np.zeros(len(dataset))  # TO DO\n",
    "        for i, data_sentence in enumerate(dataset):\n",
    "            if data_sentence:\n",
    "                sim_sentence = model.wv.n_similarity(\n",
    "                        in_vocab_list, data_sentence)\n",
    "            else:\n",
    "                sim_sentence = 0\n",
    "            sim_mat[i] = np.array(sim_sentence)\n",
    "        # Take the five highest norm\n",
    "        best_index = np.argsort(sim_mat)[::-1][:topk]\n",
    "    return best_index\n",
    "\n",
    "# Create model\n",
    "word2vec_model = Word2Vec(min_count=0, workers = 8, vector_size=300) \n",
    "# Prepare vocab\n",
    "word2vec_model.build_vocab(df.tok_lem_sentence.values)\n",
    "# Train\n",
    "word2vec_model.train(df.tok_lem_sentence.values, total_examples=word2vec_model.corpus_count, epochs=30)\n",
    "\n",
    "# Predict\n",
    "best_index = predict_w2v(test_sentence, df['tok_lem_sentence'].values, word2vec_model)    \n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "- Разберитесь с приведенными примерами, поэкспериментируйте, меняя текст в примерах\n",
    "- Разработайте программу – экспертную систему по подбору фильмов, для базы русскоязычных фильмов.\n",
    "\n",
    "https://www.kaggle.com/datasets/alexandertesemnikov/kinopoisktop250russiandataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
