{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b8026b",
   "metadata": {},
   "source": [
    "# Тематическое моделирование\n",
    "## Что такое Тематическое моделирование\n",
    "Тематическое моделирование-это неконтролируемый метод, предназначенный для анализа больших объемов текстовых данных путем кластеризации документов в группы. В случае тематического моделирования текстовые данные не имеют прикрепленных к ним меток. Скорее, тематическое моделирование пытается сгруппировать документы в кластеры на основе сходных характеристик.\n",
    "\n",
    "Типичным примером тематического моделирования является кластеризация большого количества газетных статей, относящихся к одной и той же категории. Другими словами, кластерные документы, имеющие одну и ту же тему. Здесь важно отметить, что оценить эффективность тематического моделирования крайне сложно, так как нет правильных ответов. Это зависит от пользователя, чтобы найти сходные характеристики между документами одного кластера и назначить ему соответствующую метку или тему.\n",
    "\n",
    "Для тематического моделирования в основном используются два подхода: Латентное распределение Дирихле и Неотрицательная матричная факторизация . В следующих разделах мы кратко рассмотрим оба этих подхода и посмотрим, как они могут быть применены к тематическому моделированию в Python.\n",
    "\n",
    "## Латентное распределение Дирихле (LDA)\n",
    "LDA основывается на двух общих предположениях:\n",
    "- Документы с похожими словами обычно имеют одну и ту же тему\n",
    "- Документы, в которых группы слов часто встречаются вместе, обычно имеют одну и ту же тему.\n",
    "Эти предположения имеют смысл, потому что документы, имеющие одну и ту же тему, например, бизнес-темы, будут иметь такие слова, как “экономика”, “прибыль”, “фондовый рынок”, “убыток” и т. Д. Второе предположение гласит, что если эти слова часто встречаются вместе в нескольких документах, то эти документы могут принадлежать к одной и той же категории.\n",
    "\n",
    "Математически эти два допущения можно представить в виде:\n",
    "- Документы-это распределения вероятностей по скрытым темам\n",
    "- Темы-это распределения вероятностей по словам\n",
    "## LDA для тематического моделирования в Python\n",
    "В этом разделе мы увидим, как Python может быть использован для реализации LDA для тематического моделирования. Набор данных можно загрузить с сайта Kaggle .\n",
    "Мы будем использовать тот же датасет отзывов о еде, что и в примере с сентимент-анализом (урок 4.4).\n",
    "Набор данных содержит отзывы пользователей о различных продуктах в категории продуктов питания. Мы будем использовать LDA для группировки отзывов пользователей в 5 категорий.\n",
    "\n",
    "Первым шагом, как всегда, является импорт набора данных вместе с необходимыми библиотеками. Для этого выполните следующий сценарий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318cd9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews_datasets = pd.read_csv(r'Data/Reviews.csv')\n",
    "reviews_datasets = reviews_datasets.head(20000)\n",
    "reviews_datasets.dropna()\n",
    "\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab21c0",
   "metadata": {},
   "source": [
    "В приведенном выше скрипте мы импортируем набор данных с помощью метода read_csv библиотеки pandas . Исходный набор данных содержит около 500 тысяч отзывов. Однако из-за ограничений памяти я буду выполнять LDA только на первых 20 тысячах записей. В приведенном выше скрипте мы фильтруем первые 20 тысяч строк, а затем удаляем нулевые значения из набора данных.\n",
    "\n",
    "Затем мы печатаем первые пять строк набора данных, используя функцию head() для проверки наших данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14005a",
   "metadata": {},
   "source": [
    "Мы будем применять LDA к столбцу “Текст”, так как он содержит отзывы, остальные столбцы будут проигнорированы.\n",
    "\n",
    "Давайте посмотрим обзор номер 350."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3eb378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These chocolate covered espresso beans are wonderful!  The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_datasets['Text'][350]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238223a",
   "metadata": {},
   "source": [
    "Прежде чем мы сможем применить LDA, нам нужно создать словарь всех слов в наших данных. Помните из предыдущей статьи, что мы могли бы сделать это с помощью счетчика. Посмотрите на следующий сценарий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e77314d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x14546 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 594703 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(reviews_datasets['Text'].values.astype('U'))\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf6361",
   "metadata": {},
   "source": [
    "В приведенном выше скрипте мы используем класс CountVectorizer из модуля sklearn.feature_extraction.text для создания матрицы терминов документа. Мы указываем включать только те слова, которые появляются менее чем в 80% документа и появляются как минимум в 2 документах. Мы также удаляем все стоп-слова, поскольку они на самом деле не способствуют моделированию темы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ccc7b",
   "metadata": {},
   "source": [
    "Каждый из 20 тысяч документов представлен в виде 14546-мерного вектора, что означает, что наш словарь содержит 14546 слов.\n",
    "\n",
    "Далее мы будем использовать LDA для создания тем вместе с распределением вероятностей для каждого слова в нашем словаре для каждой темы. Выполните следующий сценарий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4face1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f215acf",
   "metadata": {},
   "source": [
    "В приведенном выше скрипте мы используем класс Latent Dirichlet Allocation из библиотеки sklearn.decomposition для выполнения LDA на нашей матрице терминов документа. Параметр n_components определяет количество категорий или тем, на которые мы хотим разделить наш текст. Параметр random_state (он же seed ) имеет значение 42, так что вы получаете результаты, аналогичные моим.\n",
    "\n",
    "Давайте случайным образом выберем слова из нашего словаря. Мы знаем, что графвекторизатор содержит все слова в нашем словаре. Мы можем использовать метод get_feature_names() и передать ему идентификатор слова, которое мы хотим извлечь.\n",
    "Следующий скрипт случайным образом извлекает 10 слов из нашего словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23842aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collectors\n",
      "interfaces\n",
      "faves\n",
      "patch\n",
      "compartments\n",
      "snapping\n",
      "ka\n",
      "oreos\n",
      "scoring\n",
      "homeless\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3fbed1",
   "metadata": {},
   "source": [
    "Давайте найдем 10 слов с наибольшей вероятностью для первой темы. Чтобы получить первую тему, вы можете использовать атрибут components_ и передать индекс 0 в качестве значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01880d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.08776122 0.20082696 0.20523023 ... 0.20000269 0.20018851 0.20035394]\n"
     ]
    }
   ],
   "source": [
    "first_topic = LDA.components_[0]\n",
    "print(first_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2535298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.08776122 0.20082696 0.20523023 ... 0.20000269 0.20018851 0.20035394]\n"
     ]
    }
   ],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "print(first_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffe8e5",
   "metadata": {},
   "source": [
    "Затем эти индексы можно использовать для извлечения значения слов из объекта count_vector , что можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d564f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water\n",
      "great\n",
      "just\n",
      "drink\n",
      "sugar\n",
      "good\n",
      "flavor\n",
      "taste\n",
      "like\n",
      "tea\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a071163",
   "metadata": {},
   "source": [
    "Слова показывают, что первая тема может быть о чае.\n",
    "\n",
    "Давайте напечатаем 10 слов с наибольшей вероятностью для всех пяти тем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f1410d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['water', 'great', 'just', 'drink', 'sugar', 'good', 'flavor', 'taste', 'like', 'tea']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['br', 'chips', 'love', 'flavor', 'chocolate', 'just', 'great', 'taste', 'good', 'like']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['just', 'drink', 'orange', 'sugar', 'soda', 'water', 'like', 'juice', 'product', 'br']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['gluten', 'eat', 'free', 'product', 'like', 'dogs', 'treats', 'dog', 'br', 'food']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['cups', 'price', 'great', 'like', 'amazon', 'good', 'br', 'product', 'cup', 'coffee']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156f643",
   "metadata": {},
   "source": [
    "Вывод показывает, что вторая тема может содержать отзывы о шоколадных конфетах и т. Д. Точно так же третья тема может снова содержать отзывы о газированных напитках или соках. Вы можете видеть, что есть несколько общих слов во всех категориях. Это потому, что есть несколько слов, которые используются почти для всех тем. Например, “хорошо”, “отлично”, “нравится” и т. Д.\n",
    "\n",
    "В качестве заключительного шага мы добавим столбец в исходный фрейм данных, в котором будет храниться тема для текста. Для этого мы можем использовать метод LDA.transform() и передать ему нашу матрицу терминов документа. Этот метод назначит вероятность всех тем для каждого документа. Посмотрите на следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "788d01fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be07536",
   "metadata": {},
   "source": [
    "В выходных данных мы видим (20000, 5), что означает, что каждый документ имеет 5 столбцов, где каждый столбец соответствует значению вероятности конкретной темы. Чтобы найти индекс темы с максимальным значением, мы можем вызвать метод argmax() и передать 1 в качестве значения параметра axis.\n",
    "\n",
    "Следующий сценарий добавляет новый столбец для темы во фрейме данных и присваивает значение темы каждой строке в столбце:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409b6f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "   Topic  \n",
       "0      3  \n",
       "1      1  \n",
       "2      1  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_datasets['Topic'] = topic_values.argmax(axis=1)\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c89fd",
   "metadata": {},
   "source": [
    "Вы можете увидеть новый столбец для этой темы в выходных данных.\n",
    "\n",
    "## Неотрицательная матричная факторизация (NMF)\n",
    "В предыдущем разделе мы видели, как LDA можно использовать для тематического моделирования. В этом разделе мы увидим, как неотрицательная матричная факторизация может быть использована для тематического моделирования.\n",
    "\n",
    "Неотрицательная матричная факторизация также является контролируемым методом обучения, который выполняет кластеризацию, а также уменьшение размерности. Он может быть использован в сочетании со схемой TF-IDF для выполнения тематического моделирования. В этом разделе мы увидим, как Python можно использовать для выполнения неотрицательной матричной факторизации для тематического моделирования.\n",
    "\n",
    "## NMF для тематического моделирования в Python\n",
    "В этом разделе мы будем выполнять тематическое моделирование на том же наборе данных, что и в предыдущем разделе. Вы увидите, что шаги также очень похожи.\n",
    "\n",
    "Начнем с импорта набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "099a7260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "reviews_datasets = pd.read_csv(r'Reviews.csv')\n",
    "reviews_datasets = reviews_datasets.head(20000)\n",
    "reviews_datasets.dropna()\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ddf17",
   "metadata": {},
   "source": [
    "В предыдущем разделе мы использовали countvectorizer, но в этом разделе мы будем использовать TfidfVectorizer, так как NMF работает с TFIDF. Мы создадим матрицу терминов документа с помощью TF IDF. Посмотрите на следующий сценарий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "489ab291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vect.fit_transform(reviews_datasets['Text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ef305",
   "metadata": {},
   "source": [
    "Как только матрица терминов документа будет сгенерирована, мы можем создать матрицу вероятностей, которая содержит вероятности всех слов в словаре для всех тем. Для этого мы можем использовать класс NMF из модуля sklearn.decomposition . Посмотрите на следующий сценарий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f91302fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(doc_term_matrix )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb686e",
   "metadata": {},
   "source": [
    "Как и в предыдущем разделе, давайте случайным образом получим 10 слов из нашего словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dd4d89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freak\n",
      "ear\n",
      "yerba\n",
      "stews\n",
      "linolenic\n",
      "inconsistent\n",
      "settles\n",
      "20oz\n",
      "pickles\n",
      "commitment\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n",
    "    print(tfidf_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53199593",
   "metadata": {},
   "source": [
    "Далее мы получим вектор вероятности слов для первой темы и получим индексы десяти слов с наибольшими вероятностями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6f58d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic = nmf.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc069d3c",
   "metadata": {},
   "source": [
    "Теперь эти индексы могут быть переданы объекту tf idf_vector для извлечения фактических слов. Посмотрите на следующий сценарий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f0e9172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really\n",
      "chocolate\n",
      "love\n",
      "flavor\n",
      "just\n",
      "product\n",
      "taste\n",
      "great\n",
      "good\n",
      "like\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(tfidf_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd229868",
   "metadata": {},
   "source": [
    "Слова для темы 1 показывают, что тема 1 может содержать отзывы о шоколадных конфетах. Давайте теперь напечатаем десять слов с наибольшей вероятностью для каждой из тем:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a738cf09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['really', 'chocolate', 'love', 'flavor', 'just', 'product', 'taste', 'great', 'good', 'like']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['like', 'keurig', 'roast', 'flavor', 'blend', 'bold', 'strong', 'cups', 'cup', 'coffee']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['com', 'amazon', 'orange', 'switch', 'water', 'drink', 'soda', 'sugar', 'juice', 'br']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['bags', 'flavor', 'drink', 'iced', 'earl', 'loose', 'grey', 'teas', 'green', 'tea']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['old', 'love', 'cat', 'eat', 'treat', 'loves', 'dogs', 'food', 'treats', 'dog']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5811e7a",
   "metadata": {},
   "source": [
    "Слова для темы 1 показывают, что эта тема содержит отзывы о кофе. Аналогично, слова для темы 2 показывают, что она содержит отзывы о газированных напитках и соках. Тема 3 снова содержит отзывы о напитках. Наконец, тема 4 может содержать отзывы о животной пище, поскольку она содержит такие слова, как “кошка”, “собака”, “лакомство” и т. Д.\n",
    "\n",
    "Следующий сценарий добавляет темы в набор данных и отображает первые пять строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1de030dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "   Topic  \n",
       "0      4  \n",
       "1      0  \n",
       "2      4  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = nmf.transform(doc_term_matrix)\n",
    "reviews_datasets['Topic'] = topic_values.argmax(axis=1)\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc2405",
   "metadata": {},
   "source": [
    "Как вы можете видеть, каждому обзору была назначена тема, которая была сгенерирована с помощью метода NMF.\n",
    "\n",
    "## Вывод\n",
    "Тематическое моделирование-одно из самых востребованных направлений исследований в НЛП. Он используется для группировки больших объемов немаркированных текстовых данных. В этой статье были объяснены два подхода к тематическому моделированию. В этой статье мы увидели, как Латентное распределение Дирихле и Неотрицательная матричная факторизация могут быть использованы для тематического моделирования с помощью библиотек Python.\n",
    "\n",
    "# Домашнее задание\n",
    "1. Отработайте задания в Jupyter Notebook, разберитесь с исходными кодами\n",
    "2. Разработайте модель тематического анализа для базы данных телефонов https://www.kaggle.com/datasets/theovall/phonereviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
