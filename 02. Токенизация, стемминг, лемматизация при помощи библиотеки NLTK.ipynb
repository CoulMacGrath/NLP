{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec2704e",
   "metadata": {},
   "source": [
    "# Разбиение текста на токены\n",
    "Токенизация – процесс разбиения текста на текстовые единицы, например, слова или предложения. В случае разбиений на предложения задача кажется тривиальной, нужно просто найти точку, вопросительный или восклицательный знак. Но в русском языке существует сокращения, в которых есть точка, например, к.т.н. — кандидат технических наук или т.е. — то есть. Поэтому такой путь может привести к ошибкам. К счастью, Python-библиотека NLTK позволяет избежать этой проблемы. Рассмотрим пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2ebc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Я - к.т.н, т.е. проучился долгое время.', 'Имею образование.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Я - к.т.н, т.е. проучился долгое время. Имею образование.\"\n",
    "sent_tokenize(text, language=\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321048c",
   "metadata": {},
   "source": [
    "Как видим, функция sent_tokenize разбила исходное предложения на два, несмотря на присутствие слов к.т.н. и т.е.\n",
    "\n",
    "Помимо разбиения на предложения в NLTK можно в качестве токенов использовать слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb2b2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Я', '-', 'к.т.н.', 'Сижу', 'на', 'диван-кровати', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Я - к.т.н. Сижу на диван-кровати.\"\n",
    "word_tokenize(text, language=\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448f6f2",
   "metadata": {},
   "source": [
    "Здесь к.т.н. и диван-кровать были определены как отдельные слова.\n",
    "\n",
    "# Очистка текста от стоп-слов\n",
    "Иногда одних слов в тексте больше, чем других, к тому же они встречаются почти в каждом предложении и не несут большой информативной нагрузки. Такие слова являются шумом для последующего глубокого обучения (Deep Learning) и называются стоп-словами. Библиотека NLTK также имеет список стоп-слов, который предварительно необходимо скачать. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae43939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08beb73f",
   "metadata": {},
   "source": [
    "После этого доступен список стоп-слов для русского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0e9e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7657c4",
   "metadata": {},
   "source": [
    "Всего их насчитывается в этом списке 151. Вот некоторые из них:\n",
    "*и, в, во, не, что, он, на, я, с, со, как, а, то, все, чтоб, без, будто, впрочем, хорошо, свою, этой, перед, иногда, лучше, чуть, том, нельзя, такой, им, более, всегда, конечно, всю, между*\n",
    "\n",
    "Поскольку это список, то к нему можно добавить дополнительные слова или, наоборот, удалить из него те, которые будут информативными для вашего случая. Для последующего исключения слов из токенизированного текста можно написать следующее:\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in stop_words:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2abe7e",
   "metadata": {},
   "source": [
    "# Стемминг: удаляем окончания\n",
    "Русский язык обладает богатой морфологической структурой. Слово хороший и хорошая имеют тот же смысл, но разную форму, например, хорошая мебель и хороший стул. Поэтому для машинного обучения (Machine Learning) лучше привести их к одной форме для уменьшения размерности. Одним из таких методов является стемминг (stemming). В частности, он опускает окончания слова. В Python-библиотеке NLTK для этого есть Snowball Stemmer, который поддерживает русский язык:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ef239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хорош'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "...\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "snowball.stem(\"Хороший\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d07df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хорош'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"Хорошая\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122ccd5",
   "metadata": {},
   "source": [
    "Проблемы могут возникнуть со словами, которые значительно изменяются в зависимости от формы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6532fcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хоч'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"Хочу\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db26056d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотет'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"Хотеть\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b729c4",
   "metadata": {},
   "source": [
    "Хотеть и хочу — грамматические формы одного и то же слова, но стемминг обрубает окончания согласно своему алгоритму. Поэтому возможно следует применить другой метод — лемматизацию.\n",
    "\n",
    "# Приведение к начальной форме с лемматизацией\n",
    "Над словом можно провести морфологический анализ и выявить его начальную форму. Например, хочу, хотят, хотели имеют начальную форму хотеть. Тогда можем воспользоваться pymorphy2 — инструмент для морфологического анализа русского и украинского языков.\n",
    "\n",
    "Рассмотрим пример для слова “хочу”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c984b09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хочу', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'хочу', 3136, 1),))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"хочу\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5986e5",
   "metadata": {},
   "source": [
    "Метод parse возвращает список объектов Parse, которые обозначают виды грамматических форм анализируемого слова. Такой объект обладает следующими атрибутами:\n",
    "\n",
    "- tag обозначает набор граммем. В данном случае слово хочу — это глагол (VERB) несовершенного вида (impf), переходный (tran), единственного числа (sing), 1 лица (1per), настоящего времени (pres), изъявительного наклонения (indc);\n",
    "- normal_form— нормального форма слова;\n",
    "- score — оценка вероятности того, что данный разбор правильный;\n",
    "- methods_stack — тип словаря распарсенного слова с его индексом.\n",
    "Нас больше всего интересует нормальная форма слова. По умолчанию объекты Parse сортированы в порядке убывания значения score. Поэтому из списка лучше всего брать 1-й элемент:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cde099c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"хотеть\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81b07c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"хочу\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30166d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"хотят\")[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a40293",
   "metadata": {},
   "source": [
    "Таким образом, мы получили одно слово из разных его форм.\n",
    "\n",
    "# Домашнее задание\n",
    "- Разберитесь с приведенными примерами, поэкспериментируйте, меняя текст в примерах\n",
    "- Разработайте программу, которая выполнит для русского текста в файле .txt подготовку текста - его лемматизацию и очистку от стоп-слов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
