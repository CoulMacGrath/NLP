{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec2704e",
   "metadata": {},
   "source": [
    "# Разбиение текста на токены\n",
    "Токенизация – процесс разбиения текста на текстовые единицы, например, слова или предложения. В случае разбиений на предложения задача кажется тривиальной, нужно просто найти точку, вопросительный или восклицательный знак. Но в русском языке существует сокращения, в которых есть точка, например, к.т.н. — кандидат технических наук или т.е. — то есть. Поэтому такой путь может привести к ошибкам. К счастью, Python-библиотека NLTK позволяет избежать этой проблемы. Рассмотрим пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2ebc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Я - к.т.н, т.е. проучился долгое время.', 'Имею образование.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Я - к.т.н, т.е. проучился долгое время. Имею образование.\"\n",
    "sent_tokenize(text, language=\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321048c",
   "metadata": {},
   "source": [
    "Как видим, функция sent_tokenize разбила исходное предложения на два, несмотря на присутствие слов к.т.н. и т.е.\n",
    "\n",
    "Помимо разбиения на предложения в NLTK можно в качестве токенов использовать слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb2b2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Я', '-', 'к.т.н.', 'Сижу', 'на', 'диван-кровати', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Я - к.т.н. Сижу на диван-кровати.\"\n",
    "word_tokenize(text, language=\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448f6f2",
   "metadata": {},
   "source": [
    "Здесь к.т.н. и диван-кровать были определены как отдельные слова.\n",
    "\n",
    "# Очистка текста от стоп-слов\n",
    "Иногда одних слов в тексте больше, чем других, к тому же они встречаются почти в каждом предложении и не несут большой информативной нагрузки. Такие слова являются шумом для последующего глубокого обучения (Deep Learning) и называются стоп-словами. Библиотека NLTK также имеет список стоп-слов, который предварительно необходимо скачать. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae43939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c92a3c7-cddd-48c2-a401-4b35d74040a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymorphy3 nltk -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08beb73f",
   "metadata": {},
   "source": [
    "После этого доступен список стоп-слов для русского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0e9e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7657c4",
   "metadata": {},
   "source": [
    "Всего их насчитывается в этом списке 151. Вот некоторые из них:\n",
    "*и, в, во, не, что, он, на, я, с, со, как, а, то, все, чтоб, без, будто, впрочем, хорошо, свою, этой, перед, иногда, лучше, чуть, том, нельзя, такой, им, более, всегда, конечно, всю, между*\n",
    "\n",
    "Поскольку это список, то к нему можно добавить дополнительные слова или, наоборот, удалить из него те, которые будут информативными для вашего случая. Для последующего исключения слов из токенизированного текста можно написать следующее:\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in stop_words:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2abe7e",
   "metadata": {},
   "source": [
    "# Стемминг: удаляем окончания\n",
    "Русский язык обладает богатой морфологической структурой. Слово хороший и хорошая имеют тот же смысл, но разную форму, например, хорошая мебель и хороший стул. Поэтому для машинного обучения (Machine Learning) лучше привести их к одной форме для уменьшения размерности. Одним из таких методов является стемминг (stemming). В частности, он опускает окончания слова. В Python-библиотеке NLTK для этого есть Snowball Stemmer, который поддерживает русский язык:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ef239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хорош'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "...\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "snowball.stem(\"Хороший\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d07df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хорош'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"Хорошая\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122ccd5",
   "metadata": {},
   "source": [
    "Проблемы могут возникнуть со словами, которые значительно изменяются в зависимости от формы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6532fcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хоч'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"Хочу\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db26056d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотет'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"Хотеть\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b729c4",
   "metadata": {},
   "source": [
    "Хотеть и хочу — грамматические формы одного и то же слова, но стемминг обрубает окончания согласно своему алгоритму. Поэтому возможно следует применить другой метод — лемматизацию.\n",
    "\n",
    "# Приведение к начальной форме с лемматизацией\n",
    "Над словом можно провести морфологический анализ и выявить его начальную форму. Например, хочу, хотят, хотели имеют начальную форму хотеть. Тогда можем воспользоваться pymorphy2 — инструмент для морфологического анализа русского и украинского языков.\n",
    "\n",
    "Рассмотрим пример для слова “хочу”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c984b09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хочу', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'хочу', 3136, 1),))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "morph.parse(\"хочу\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5986e5",
   "metadata": {},
   "source": [
    "Метод parse возвращает список объектов Parse, которые обозначают виды грамматических форм анализируемого слова. Такой объект обладает следующими атрибутами:\n",
    "\n",
    "- tag обозначает набор граммем. В данном случае слово хочу — это глагол (VERB) несовершенного вида (impf), переходный (tran), единственного числа (sing), 1 лица (1per), настоящего времени (pres), изъявительного наклонения (indc);\n",
    "- normal_form— нормального форма слова;\n",
    "- score — оценка вероятности того, что данный разбор правильный;\n",
    "- methods_stack — тип словаря распарсенного слова с его индексом.\n",
    "Нас больше всего интересует нормальная форма слова. По умолчанию объекты Parse сортированы в порядке убывания значения score. Поэтому из списка лучше всего брать 1-й элемент:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse(\"хотеть\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse(\"хочу\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30166d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse(\"хотят\")[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a40293",
   "metadata": {},
   "source": [
    "Таким образом, мы получили одно слово из разных его форм.\n",
    "\n",
    "# Домашнее задание\n",
    "- Разберитесь с приведенными примерами, поэкспериментируйте, меняя текст в примерах\n",
    "- Разработайте программу, которая выполнит для русского текста в файле .txt подготовку текста - его лемматизацию и очистку от стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e9a4da-0f5f-4eff-9e41-1bc21dd192aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено в Data/властелин_колец2.txt\n",
      "мистер бильб бэггинс бэг-энд объяв вскор празднова сво сто одиннадцат ден рожден вечеринк особ пышност хоббитон подня разговор волнен бильб очен богат очен своеобразн шестьдес год диковинк шир тот сам пор таинствен исчезнут неожида вернут богатств котор привезт сво странств стат местн легенд народ вер холм бэг-энд полн тоннел набит сокровищ говор старик недостаточн слав ещ неувяда бодрост вызыва удивлен врем идт мистер бэггинс он каза отража девян год так пятьдес исполн девян девя стат называ сохран неизмен близк истин некотор кача голов счита эт слишк каза несправедлив кто-т облада каза вечн юност вмест слух неисчерпа богатств эт прийт заплат говор эт ненормальн быт неприятн пок неприятн поскольк мистер бэггинс щедр трат сво деньг большинств охотн проща странност удач поддержива отношен родственник исключен сэквилль-бэггинс преда почитател сред хобб бедн незнатн сем близк друг пок некотор младш куз подраст старш любимец бильб юн фрод бэггинс бильб исполн девян девя усынов фрод сво наследник привезт жит бэг-энд надежд сэквилль-бэггинс окончательн рухнут бильб фрод случайн род ден сентябр ты перееха сюд жит фрод мальчик сказа однажд бильб смоч спокойн празднова наш ден рожден вмест врем фрод ещ двадцатк хобб называ беззаботн год детств совершеннолет тридца год пройт ещ двенадца год кажд год бэггинс устраива очен оживлен совместн ден рожден бэг-энд понима осен планирова нечт совершен исключительн бильб собира отпразднова сто одиннадца год 111 довольн любопытн числ весьм почтен возраст хобб стар тук дож 130 фрод исполня тридца важн числ возраст совершеннолет хобб\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy3\n",
    "import re\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Обработка текста\"\"\"\n",
    "    words = word_tokenize(text.lower(), language=\"russian\")\n",
    "    stop_words = set(stopwords.words(\"russian\"))\n",
    "    \n",
    "    filtered = []\n",
    "    for word in words:\n",
    "        if not re.match(r'^[^\\w\\s-]+$', word):\n",
    "            if word not in stop_words and len(word) > 2:\n",
    "                filtered.append(word)\n",
    "    snowball = SnowballStemmer(language=\"russian\")\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lemmas = []\n",
    "    \n",
    "    for word in filtered:\n",
    "        try:\n",
    "            lemma = morph.parse(word)[0].normal_form\n",
    "            lemma = snowball.stem(lemma)\n",
    "            lemmas.append(lemma)\n",
    "        except: \n",
    "            lemmas.append(word)\n",
    "    \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def process_file(input_file, output_file=None):\n",
    "    \"\"\"Обработка файла\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    result = process_text(text)\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(result)\n",
    "        print(f\"Сохранено в {output_file}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if len(sys.argv) > 1:\n",
    "        result = process_file('Data/властелин_колец.txt', 'Data/властелин_колец2.txt' if len(sys.argv) > 2 else None)\n",
    "        print(result)\n",
    "    else:\n",
    "        print(\"Использование: python text_processor.py входной_файл [выходной_файл]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e41ce3-da66-433f-9c56-1ef28d94c31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
