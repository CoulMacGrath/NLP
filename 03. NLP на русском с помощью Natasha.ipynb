{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8755f6d-db7d-4b27-827b-c1a7e3a842d3",
   "metadata": {},
   "source": [
    "# Библиотека Natasha для NLP на русском\n",
    "\n",
    "`Natasha` решает базовые задачи NLP для русского языка: токенизацию, сегментацию предложений, векторное представление слов (word embedding), морфологическую разметку, лемматизацию, нормализацию фраз, синтаксический анализ, извлечение именованных сущностей (NER) и фактов. Качество выполнения каждой задачи сопоставимо или превосходит текущие state-of-the-art (SOTA) решения для русского языка на новостных статьях (см. раздел с оценкой).  \n",
    "`Natasha` — не исследовательский проект, базовые технологии созданы для промышленного использования. Модели работают на CPU и используют Numpy для вывода.  \n",
    "\n",
    "`Natasha` объединяет библиотеки проекта Natasha в единый удобный API:  \n",
    "- **Razdel** — токенизация и сегментация предложений для русского языка  \n",
    "- **Navec** — компактные векторные представления для русского языка  \n",
    "- **Slovnet** — современные методы deep learning для русского NLP, компактные модели для морфологии, синтаксиса и NER  \n",
    "- **Yargy** — правило-ориентированное извлечение фактов (аналогично парсеру Томита)  \n",
    "- **Ipymarkup** — визуализация NLP-разметки (NER и синтаксис)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6502c7-815e-4558-bbf9-06f6e3a9ee34",
   "metadata": {},
   "source": [
    "## Установка\n",
    "\n",
    "`Natasha` требует Python 3.7+ and PyPy3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217f378-a615-492e-b9d5-68e3352d3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd402522-9455-4ecb-a669-5d2f149a760f",
   "metadata": {},
   "source": [
    "## Использование\n",
    "\n",
    "Импортируйте, инициализируйте модули, создайте объект Doc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d13276-0820-4ba7-b1ba-9a13aec45fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых модулей из библиотеки Natasha\n",
    "# Natasha - это библиотека для обработки естественного языка (NLP) для русского языка\n",
    "from natasha import (\n",
    "    Segmenter,          # Разбивает текст на предложения и токены (слова, знаки препинания)\n",
    "    MorphVocab,         # Морфологический словарь для нормализации слов\n",
    "    \n",
    "    # Модели на основе предобученных эмбеддингов\n",
    "    NewsEmbedding,      # Предобученные векторные представления слов для новостных текстов\n",
    "    NewsMorphTagger,    # Морфологический анализатор (часть речи, род, число и др.)\n",
    "    NewsSyntaxParser,   # Синтаксический анализатор (зависимости между словами)\n",
    "    NewsNERTagger,      # Распознавание именованных сущностей (персоны, организации и др.)\n",
    "    \n",
    "    PER,                # Константа для обозначения типа сущности \"Персона\"\n",
    "    NamesExtractor,     # Извлечение и нормализация имен\n",
    "\n",
    "    Doc                 # Основной класс для работы с документами\n",
    ")\n",
    "\n",
    "# Инициализация компонентов Natasha:\n",
    "\n",
    "# Сегментатор - разбивает текст на токены (слова) и предложения\n",
    "segmenter = Segmenter()\n",
    "\n",
    "# Морфологический словарь - используется для нормализации слов (приведение к начальной форме)\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "# Загрузка предобученных эмбеддингов (векторных представлений слов)\n",
    "emb = NewsEmbedding()\n",
    "\n",
    "# Инициализация моделей на основе загруженных эмбеддингов:\n",
    "\n",
    "# Морфологический теггер - определяет часть речи и грамматические характеристики\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "# Синтаксический парсер - анализирует грамматическую структуру предложения\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "\n",
    "# NER (Named Entity Recognition) теггер - распознает именованные сущности\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "# Инициализация экстрактора имен для нормализации имен собственных\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "# Пример текста для анализа - немного классики\n",
    "text = 'Наш хоббит был весьма состоятельным хоббитом по фамилии Бэггинс. Бэггинсы проживали в окрестностях Холма с незапамятных времен и считались очень почтенным семейством не только потому, что были богаты, но и потому, что с ними никогда и ничего не приключалось и они не позволяли себе ничего неожиданного: всегда можно было угадать заранее, не спрашивая, что именно скажет тот или иной Бэггинс по тому или иному поводу. Но мы вам поведаем историю о том, как одного из Бэггинсов втянули-таки в приключения и, к собственному удивлению, он начал говорить самые неожиданные вещи и совершать самые неожиданные поступки. Может быть, он и потерял уважение соседей, но зато приобрел… впрочем, увидите сами, приобрел он в конце концов или нет. Матушка нашего хоббита… кстати, кто такой хоббит? Пожалуй, стоит рассказать о хоббитах подробнее, так как в наше время они стали редкостью и сторонятся Высокого Народа, как они называют нас, людей. Сами они низкорослый народец, примерно в половину нашего роста и пониже бородатых гномов. Бороды у хоббитов нет. Волшебного в них тоже, в общем-то, ничего нет, если не считать волшебным умение быстро и бесшумно исчезать в тех случаях, когда всякие бестолковые, неуклюжие верзилы, вроде нас с вами, с шумом и треском ломятся, как слоны. У хоббитов толстенькое брюшко; одеваются они ярко, преимущественно в зеленое и желтое; башмаков не носят, потому что на ногах у них от природы жесткие кожаные подошвы и густой теплый бурый мех, как и на голове. Только на голове он курчавится. У хоббитов длинные ловкие темные пальцы на руках, добродушные лица; смеются они густым утробным смехом (особенно после обеда, а обедают они, как правило дважды в день, если получится).'\n",
    "\n",
    "# Создание объекта Doc - основной объект для работы с текстом в Natasha\n",
    "# При инициализации текст автоматически сегментируется на токены и предложения\n",
    "doc = Doc(text)\n",
    "\n",
    "# Далее обычно выполняются следующие шаги (хотя в этом примере они не показаны):\n",
    "# 1. doc.segment(segmenter) - сегментация (уже выполнена при создании Doc)\n",
    "# 2. doc.tag_morph(morph_tagger) - морфологический разбор\n",
    "# 3. doc.parse_syntax(syntax_parser) - синтаксический разбор\n",
    "# 4. doc.tag_ner(ner_tagger) - извлечение именованных сущностей\n",
    "# 5. Обработка результатов, например, нормализация имен с помощью names_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba60c01-543a-48ad-9f40-68fb5c46be13",
   "metadata": {},
   "source": [
    "## Сегментация  \n",
    "Разделяет текст на токены и предложения. Определяет свойства tokens и sents документа. Использует библиотеку `Razdel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c837e0-b8bc-4f30-b2ee-74a7cec63548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=3, text='Наш'), DocToken(start=4, stop=10, text='хоббит'), DocToken(start=11, stop=14, text='был'), DocToken(start=15, stop=21, text='весьма'), DocToken(start=22, stop=35, text='состоятельным')]\n",
      "[DocSent(stop=64, text='Наш хоббит был весьма состоятельным хоббитом по ф..., tokens=[...]), DocSent(start=65, stop=416, text='Бэггинсы проживали в окрестностях Холма с незапам..., tokens=[...]), DocSent(start=417, stop=611, text='Но мы вам поведаем историю о том, как одного из Б..., tokens=[...]), DocSent(start=612, stop=731, text='Может быть, он и потерял уважение соседей, но зат..., tokens=[...]), DocSent(start=732, stop=781, text='Матушка нашего хоббита… кстати, кто такой хоббит?..., tokens=[...])]\n"
     ]
    }
   ],
   "source": [
    "# Применяем сегментатор к документу (разбиваем текст на предложения и токены)\n",
    "# segmenter - это объект, который содержит правила для разбиения текста\n",
    "# doc.segment() модифицирует документ, добавляя информацию о границах предложений и токенов\n",
    "doc.segment(segmenter)\n",
    "\n",
    "# Выводим первые 5 токенов документа\n",
    "# doc.tokens - это список токенов (отдельных слов, знаков препинания и т.д.)\n",
    "# [:5] - срез списка, который берет первые 5 элементов\n",
    "print(doc.tokens[:5])\n",
    "\n",
    "# Выводим первые 5 предложений документа\n",
    "# doc.sents - это список предложений документа\n",
    "# Каждое предложение представлено как последовательность токенов\n",
    "print(doc.sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2cc51e-c906-4938-9103-e4df8cb325c5",
   "metadata": {},
   "source": [
    "## Морфология  \n",
    "Для каждого токена извлекаются расширенные морфологические признаки. Зависит от этапа сегментации. Определяет свойства pos (часть речи) и feats (грамматические характеристики) для doc.tokens. Внутри использует морфологическую модель Slovnet.  \n",
    "\n",
    "Вызовите morph.print(), чтобы визуализировать морфологическую разметку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9c33e7-2ddd-4f83-82f9-7044c6a2888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=3, text='Наш', pos='DET', feats=<Nom,Masc,Sing>), DocToken(start=4, stop=10, text='хоббит', pos='NOUN', feats=<Inan,Nom,Masc,Sing>), DocToken(start=11, stop=14, text='был', pos='AUX', feats=<Imp,Masc,Ind,Sing,Past,Fin,Act>), DocToken(start=15, stop=21, text='весьма', pos='ADV', feats=<Pos>), DocToken(start=22, stop=35, text='состоятельным', pos='ADJ', feats=<Ins,Pos,Masc,Sing>)]\n",
      "                 Наш DET|Case=Nom|Gender=Masc|Number=Sing\n",
      "              хоббит NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "                 был AUX|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\n",
      "              весьма ADV|Degree=Pos\n",
      "       состоятельным ADJ|Case=Ins|Degree=Pos|Gender=Masc|Number=Sing\n",
      "            хоббитом NOUN|Animacy=Anim|Case=Ins|Gender=Masc|Number=Sing\n",
      "                  по ADP\n",
      "             фамилии NOUN|Animacy=Inan|Case=Dat|Gender=Fem|Number=Sing\n",
      "             Бэггинс PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
      "                   . PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Применяем морфологический анализатор (тэггер) к документу doc\n",
    "# morph_tagger - это объект, который содержит правила или модель для разбора морфологии\n",
    "# Метод tag_morph проходит по всем токенам в документе и добавляет к ним морфологические разборы\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "# Выводим первые 5 токенов документа после морфологического разбора\n",
    "# Это полезно для проверки, что разбор был применён корректно\n",
    "# Каждый токен будет содержать информацию о лемме, части речи, грамматических признаках и т.д.\n",
    "print(doc.tokens[:5])\n",
    "\n",
    "# Обращаемся к первому предложению в документе (doc.sents[0])\n",
    "# Вызываем метод .morph, который возвращает морфологическую информацию предложения,\n",
    "# и затем .print() для красивого вывода этой информации в консоль\n",
    "# Это покажет все морфологические разборы для каждого токена в предложении в удобном формате\n",
    "doc.sents[0].morph.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a29b1-62fa-488f-8fa4-4fe82cabb871",
   "metadata": {},
   "source": [
    "## Лемматизация  \n",
    "\n",
    "Лемматизировать каждый токен. Зависит от этапа морфологического анализа. Определяет свойство lemma для doc.tokens. Внутри использует Pymorphy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "442c0530-4998-4e9f-95c5-8573f0b0cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=3, text='Наш', pos='DET', feats=<Nom,Masc,Sing>, lemma='наш'), DocToken(start=4, stop=10, text='хоббит', pos='NOUN', feats=<Inan,Nom,Masc,Sing>, lemma='хоббит'), DocToken(start=11, stop=14, text='был', pos='AUX', feats=<Imp,Masc,Ind,Sing,Past,Fin,Act>, lemma='быть'), DocToken(start=15, stop=21, text='весьма', pos='ADV', feats=<Pos>, lemma='весьма'), DocToken(start=22, stop=35, text='состоятельным', pos='ADJ', feats=<Ins,Pos,Masc,Sing>, lemma='состоятельный')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Наш': 'наш',\n",
       " 'хоббит': 'хоббит',\n",
       " 'был': 'быть',\n",
       " 'весьма': 'весьма',\n",
       " 'состоятельным': 'состоятельный',\n",
       " 'хоббитом': 'хоббит',\n",
       " 'по': 'по',\n",
       " 'фамилии': 'фамилия',\n",
       " 'Бэггинс': 'бэггинс',\n",
       " '.': '.',\n",
       " 'Бэггинсы': 'бэггинс',\n",
       " 'проживали': 'проживать',\n",
       " 'в': 'в',\n",
       " 'окрестностях': 'окрестность',\n",
       " 'Холма': 'холм',\n",
       " 'с': 'с',\n",
       " 'незапамятных': 'незапамятных',\n",
       " 'времен': 'время',\n",
       " 'и': 'и',\n",
       " 'считались': 'считаться',\n",
       " 'очень': 'очень',\n",
       " 'почтенным': 'почтенный',\n",
       " 'семейством': 'семейство',\n",
       " 'не': 'не',\n",
       " 'только': 'только',\n",
       " 'потому': 'потому',\n",
       " ',': ',',\n",
       " 'что': 'что',\n",
       " 'были': 'быть',\n",
       " 'богаты': 'богатый',\n",
       " 'но': 'но',\n",
       " 'ними': 'они',\n",
       " 'никогда': 'никогда',\n",
       " 'ничего': 'ничто',\n",
       " 'приключалось': 'приключалось',\n",
       " 'они': 'они',\n",
       " 'позволяли': 'позволять',\n",
       " 'себе': 'себя',\n",
       " 'неожиданного': 'неожиданный',\n",
       " ':': ':',\n",
       " 'всегда': 'всегда',\n",
       " 'можно': 'можно',\n",
       " 'было': 'быть',\n",
       " 'угадать': 'угадать',\n",
       " 'заранее': 'заранее',\n",
       " 'спрашивая': 'спрашивать',\n",
       " 'именно': 'именно',\n",
       " 'скажет': 'сказать',\n",
       " 'тот': 'тот',\n",
       " 'или': 'или',\n",
       " 'иной': 'иной',\n",
       " 'тому': 'тот',\n",
       " 'иному': 'иной',\n",
       " 'поводу': 'повод',\n",
       " 'Но': 'но',\n",
       " 'мы': 'мы',\n",
       " 'вам': 'вы',\n",
       " 'поведаем': 'поведать',\n",
       " 'историю': 'история',\n",
       " 'о': 'о',\n",
       " 'том': 'тот',\n",
       " 'как': 'как',\n",
       " 'одного': 'один',\n",
       " 'из': 'из',\n",
       " 'Бэггинсов': 'бэггинс',\n",
       " 'втянули-таки': 'втянули-таки',\n",
       " 'приключения': 'приключение',\n",
       " 'к': 'к',\n",
       " 'собственному': 'собственный',\n",
       " 'удивлению': 'удивление',\n",
       " 'он': 'он',\n",
       " 'начал': 'начать',\n",
       " 'говорить': 'говорить',\n",
       " 'самые': 'самый',\n",
       " 'неожиданные': 'неожиданный',\n",
       " 'вещи': 'вещь',\n",
       " 'совершать': 'совершать',\n",
       " 'поступки': 'поступок',\n",
       " 'Может': 'мочь',\n",
       " 'быть': 'быть',\n",
       " 'потерял': 'потерять',\n",
       " 'уважение': 'уважение',\n",
       " 'соседей': 'сосед',\n",
       " 'зато': 'зато',\n",
       " 'приобрел': 'приобрести',\n",
       " '…': '…',\n",
       " 'впрочем': 'впрочем',\n",
       " 'увидите': 'увидеть',\n",
       " 'сами': 'сам',\n",
       " 'конце': 'конец',\n",
       " 'концов': 'конец',\n",
       " 'нет': 'нет',\n",
       " 'Матушка': 'матушка',\n",
       " 'нашего': 'наш',\n",
       " 'хоббита': 'хоббит',\n",
       " 'кстати': 'кстати',\n",
       " 'кто': 'кто',\n",
       " 'такой': 'такой',\n",
       " '?': '?',\n",
       " 'Пожалуй': 'пожалуй',\n",
       " 'стоит': 'стоить',\n",
       " 'рассказать': 'рассказать',\n",
       " 'хоббитах': 'хоббит',\n",
       " 'подробнее': 'подробный',\n",
       " 'так': 'так',\n",
       " 'наше': 'наш',\n",
       " 'время': 'время',\n",
       " 'стали': 'стать',\n",
       " 'редкостью': 'редкость',\n",
       " 'сторонятся': 'сторониться',\n",
       " 'Высокого': 'высокий',\n",
       " 'Народа': 'народ',\n",
       " 'называют': 'называть',\n",
       " 'нас': 'мы',\n",
       " 'людей': 'человек',\n",
       " 'Сами': 'сам',\n",
       " 'низкорослый': 'низкорослый',\n",
       " 'народец': 'народец',\n",
       " 'примерно': 'примерно',\n",
       " 'половину': 'половина',\n",
       " 'роста': 'рост',\n",
       " 'пониже': 'пониже',\n",
       " 'бородатых': 'бородатый',\n",
       " 'гномов': 'гном',\n",
       " 'Бороды': 'борода',\n",
       " 'у': 'у',\n",
       " 'хоббитов': 'хоббит',\n",
       " 'Волшебного': 'волшебный',\n",
       " 'них': 'они',\n",
       " 'тоже': 'тоже',\n",
       " 'общем-то': 'общий-то',\n",
       " 'если': 'если',\n",
       " 'считать': 'считать',\n",
       " 'волшебным': 'волшебный',\n",
       " 'умение': 'умение',\n",
       " 'быстро': 'быстро',\n",
       " 'бесшумно': 'бесшумно',\n",
       " 'исчезать': 'исчезать',\n",
       " 'тех': 'тот',\n",
       " 'случаях': 'случай',\n",
       " 'когда': 'когда',\n",
       " 'всякие': 'всякий',\n",
       " 'бестолковые': 'бестолковый',\n",
       " 'неуклюжие': 'неуклюжий',\n",
       " 'верзилы': 'верзила',\n",
       " 'вроде': 'вроде',\n",
       " 'вами': 'вы',\n",
       " 'шумом': 'шум',\n",
       " 'треском': 'треск',\n",
       " 'ломятся': 'ломиться',\n",
       " 'слоны': 'слон',\n",
       " 'У': 'у',\n",
       " 'толстенькое': 'толстенький',\n",
       " 'брюшко': 'брюшко',\n",
       " ';': ';',\n",
       " 'одеваются': 'одеваться',\n",
       " 'ярко': 'ярко',\n",
       " 'преимущественно': 'преимущественно',\n",
       " 'зеленое': 'зеленый',\n",
       " 'желтое': 'желтый',\n",
       " 'башмаков': 'башмаков',\n",
       " 'носят': 'носить',\n",
       " 'на': 'на',\n",
       " 'ногах': 'нога',\n",
       " 'от': 'от',\n",
       " 'природы': 'природа',\n",
       " 'жесткие': 'жесткий',\n",
       " 'кожаные': 'кожаный',\n",
       " 'подошвы': 'подошва',\n",
       " 'густой': 'густой',\n",
       " 'теплый': 'теплый',\n",
       " 'бурый': 'бурый',\n",
       " 'мех': 'мех',\n",
       " 'голове': 'голова',\n",
       " 'Только': 'только',\n",
       " 'курчавится': 'курчавиться',\n",
       " 'длинные': 'длинный',\n",
       " 'ловкие': 'ловкий',\n",
       " 'темные': 'темный',\n",
       " 'пальцы': 'палец',\n",
       " 'руках': 'рука',\n",
       " 'добродушные': 'добродушный',\n",
       " 'лица': 'лицо',\n",
       " 'смеются': 'смеяться',\n",
       " 'густым': 'густой',\n",
       " 'утробным': 'утробный',\n",
       " 'смехом': 'смех',\n",
       " '(': '(',\n",
       " 'особенно': 'особенно',\n",
       " 'после': 'после',\n",
       " 'обеда': 'обед',\n",
       " 'а': 'а',\n",
       " 'обедают': 'обедать',\n",
       " 'правило': 'правило',\n",
       " 'дважды': 'дважды',\n",
       " 'день': 'день',\n",
       " 'получится': 'получиться',\n",
       " ')': ')'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Итерируемся по всем токенам в документе doc\n",
    "# doc.tokens - это список токенов (слов, знаков препинания и т.д.), полученных после обработки текста\n",
    "for token in doc.tokens:\n",
    "    # Применяем лемматизацию к текущему токену\n",
    "    # Лемматизация - приведение слова к его нормальной (словарной) форме\n",
    "    # Например: \"бежал\" → \"бежать\", \"машины\" → \"машина\"\n",
    "    # morph_vocab - это морфологический словарь, используемый для лемматизации\n",
    "    token.lemmatize(morph_vocab)\n",
    "    \n",
    "# Выводим первые 5 токенов документа после лемматизации\n",
    "# Это полезно для проверки результатов обработки\n",
    "print(doc.tokens[:5])\n",
    "\n",
    "# Создаем словарь, где ключами являются исходные тексты токенов,\n",
    "# а значениями - их лемматизированные формы\n",
    "# Используем генератор словаря для компактной записи\n",
    "# _ - это временная переменная, представляющая каждый токен в doc.tokens\n",
    "{_.text: _.lemma for _ in doc.tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f4679-ecc7-40ba-b983-9c9deda43e86",
   "metadata": {},
   "source": [
    "## Синтаксический анализ  \n",
    "\n",
    "Для каждого предложения запускается синтаксический анализатор. Зависит от этапа сегментации. Определяет свойства id, head_id, rel для токенов в doc.tokens. Внутри использует модель синтаксического анализа Slovnet.  \n",
    "\n",
    "Используйте syntax.print(), чтобы визуализировать синтаксическую разметку. Внутри применяется Ipymarkup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd33e73a-8151-4e92-9464-c238fe39c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=3, text='Наш', id='1_1', head_id='1_2', rel='det', pos='DET', feats=<Nom,Masc,Sing>, lemma='наш'), DocToken(start=4, stop=10, text='хоббит', id='1_2', head_id='1_6', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Masc,Sing>, lemma='хоббит'), DocToken(start=11, stop=14, text='был', id='1_3', head_id='1_6', rel='cop', pos='AUX', feats=<Imp,Masc,Ind,Sing,Past,Fin,Act>, lemma='быть'), DocToken(start=15, stop=21, text='весьма', id='1_4', head_id='1_5', rel='advmod', pos='ADV', feats=<Pos>, lemma='весьма'), DocToken(start=22, stop=35, text='состоятельным', id='1_5', head_id='1_6', rel='amod', pos='ADJ', feats=<Ins,Pos,Masc,Sing>, lemma='состоятельный')]\n",
      "        ┌► Наш           det\n",
      "  ┌────►└─ хоббит        nsubj\n",
      "  │ ┌────► был           cop\n",
      "  │ │   ┌► весьма        advmod\n",
      "  │ │ ┌►└─ состоятельным amod\n",
      "┌─└─└─└─── хоббитом      \n",
      "│   │   ┌► по            case\n",
      "│   └►┌─└─ фамилии       nmod\n",
      "│     └──► Бэггинс       appos\n",
      "└────────► .             punct\n"
     ]
    }
   ],
   "source": [
    "# Анализ синтаксической структуры документа с использованием указанного синтаксического парсера\n",
    "# syntax_parser - это объект парсера, который содержит правила и модели для разбора синтаксиса\n",
    "# (например, Stanford Parser, spaCy, SyntaxNet или другой NLP-инструмент)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "\n",
    "# Вывод первых 5 токенов документа после синтаксического разбора\n",
    "# Токены - это минимальные значимые единицы текста (слова, знаки препинания и т.д.)\n",
    "# [:5] - срез, берущий первые 5 элементов списка\n",
    "print(doc.tokens[:5])\n",
    "\n",
    "# Обращение к первому предложению в документе (индекс 0)\n",
    "# doc.sents - это список предложений в документе, где каждое предложение - это объект с различными атрибутами\n",
    "# .syntax - атрибут предложения, содержащий его синтаксическую структуру\n",
    "# .print() - метод для вывода наглядного представления синтаксического дерева предложения\n",
    "doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb6f97-6abf-4c7e-8ce0-6af637a0db6a",
   "metadata": {},
   "source": [
    "## NER (\"Named Entity Recognition\")\n",
    "\n",
    "Извлекает стандартные именованные сущности: имена, локации, организации. Зависит от этапа сегментации. Определяет свойство spans документа. Внутри использует модель NER от Slovnet.  \n",
    "\n",
    "Вызовите ner.print(), чтобы визуализировать NER-разметку. Внутри используется `Ipymarkup`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4a086ff-c304-48f8-bcf3-0b78f0873f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSpan(start=56, stop=63, type='PER', text='Бэггинс', tokens=[...]), DocSpan(start=65, stop=73, type='PER', text='Бэггинсы', tokens=[...]), DocSpan(start=99, stop=104, type='LOC', text='Холма', tokens=[...]), DocSpan(start=383, stop=390, type='PER', text='Бэггинс', tokens=[...]), DocSpan(start=465, stop=474, type='LOC', text='Бэггинсов', tokens=[...])]\n",
      "Наш хоббит был весьма состоятельным хоббитом по фамилии Бэггинс. \n",
      "                                                        PER────  \n",
      "Бэггинсы проживали в окрестностях Холма с незапамятных времен и \n",
      "PER─────                          LOC──                         \n",
      "считались очень почтенным семейством не только потому, что были \n",
      "богаты, но и потому, что с ними никогда и ничего не приключалось и они\n",
      " не позволяли себе ничего неожиданного: всегда можно было угадать \n",
      "заранее, не спрашивая, что именно скажет тот или иной Бэггинс по тому \n",
      "                                                      PER────         \n",
      "или иному поводу. Но мы вам поведаем историю о том, как одного из \n",
      "Бэггинсов втянули-таки в приключения и, к собственному удивлению, он \n",
      "LOC──────                                                            \n",
      "начал говорить самые неожиданные вещи и совершать самые неожиданные \n",
      "поступки. Может быть, он и потерял уважение соседей, но зато приобрел…\n",
      " впрочем, увидите сами, приобрел он в конце концов или нет. Матушка \n",
      "нашего хоббита… кстати, кто такой хоббит? Пожалуй, стоит рассказать о \n",
      "хоббитах подробнее, так как в наше время они стали редкостью и \n",
      "сторонятся Высокого Народа, как они называют нас, людей. Сами они \n",
      "           ORG────────────                                        \n",
      "низкорослый народец, примерно в половину нашего роста и пониже \n",
      "бородатых гномов. Бороды у хоббитов нет. Волшебного в них тоже, в \n",
      "общем-то, ничего нет, если не считать волшебным умение быстро и \n",
      "бесшумно исчезать в тех случаях, когда всякие бестолковые, неуклюжие \n",
      "верзилы, вроде нас с вами, с шумом и треском ломятся, как слоны. У \n",
      "хоббитов толстенькое брюшко; одеваются они ярко, преимущественно в \n",
      "зеленое и желтое; башмаков не носят, потому что на ногах у них от \n",
      "природы жесткие кожаные подошвы и густой теплый бурый мех, как и на \n",
      "голове. Только на голове он курчавится. У хоббитов длинные ловкие \n",
      "темные пальцы на руках, добродушные лица; смеются они густым утробным \n",
      "смехом (особенно после обеда, а обедают они, как правило дважды в \n",
      "день, если получится).\n"
     ]
    }
   ],
   "source": [
    "# Применяем модель NER (Named Entity Recognition - распознавание именованных сущностей) к документу\n",
    "# ner_tagger - это предварительно загруженная модель для извлечения именованных сущностей\n",
    "# Метод tag_ner() добавляет аннотации именованных сущностей к документу\n",
    "doc.tag_ner(ner_tagger)\n",
    "\n",
    "# Выводим первые 5 распознанных именованных сущностей из документа\n",
    "# doc.spans содержит все распознанные сущности в виде списка объектов Span\n",
    "# Каждый Span содержит информацию о типе сущности (PER, ORG, LOC и т.д.) и её позиции в тексте\n",
    "print(doc.spans[:5])\n",
    "\n",
    "# Выводим статистическую информацию о распознанных именованных сущностях\n",
    "# Метод print() объекта ner показывает:\n",
    "# - общее количество распознанных сущностей\n",
    "# - распределение по типам сущностей\n",
    "# - другие статистические данные о результатах NER\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20f1ac-7455-4d7b-891b-cd56b9d1d8a4",
   "metadata": {},
   "source": [
    "## Нормализация именованных сущностей  \n",
    "Для каждого NER-спана применяется процедура нормализации. Зависит от этапов NER, морфологии и синтаксиса. Определяет свойство normal для doc.spans.  \n",
    "\n",
    "Нельзя просто лемматизировать каждое слово внутри спана сущности, иначе «Бороды у хоббитов нет» превратится в «Борода у хоббит нет». Natasha использует синтаксические зависимости, чтобы получить корректный вариант: «Бороды у хоббитов нет».  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036d4088-e47d-447f-88a9-afe79ba0f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSpan(start=56, stop=63, type='PER', text='Бэггинс', tokens=[...], normal='Бэггинс'), DocSpan(start=65, stop=73, type='PER', text='Бэггинсы', tokens=[...], normal='Бэггинсы'), DocSpan(start=99, stop=104, type='LOC', text='Холма', tokens=[...], normal='Холм'), DocSpan(start=383, stop=390, type='PER', text='Бэггинс', tokens=[...], normal='Бэггинс'), DocSpan(start=465, stop=474, type='LOC', text='Бэггинсов', tokens=[...], normal='Бэггинсов')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Холма': 'Холм', 'Высокого Народа': 'Высокий Народ'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Итерируемся по всем спанам (span) в документе doc\n",
    "# Спан - это последовательность токенов, которая может представлять именованную сущность или другой значимый фрагмент текста\n",
    "for span in doc.spans:\n",
    "    # Нормализуем каждый спан с помощью морфологического словаря morph_vocab\n",
    "    # Нормализация приводит слово к его начальной форме (лемматизация) или стандартному виду\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "# Выводим первые 5 спанов документа после нормализации\n",
    "# Это помогает проверить, как прошла нормализация на небольшом примере\n",
    "print(doc.spans[:5])\n",
    "\n",
    "# Создаем словарь, где ключи - оригинальные тексты спанов,\n",
    "# а значения - их нормализованные формы\n",
    "# Включаем в словарь только те спаны, где оригинальный текст отличается от нормализованного\n",
    "# Это позволяет увидеть только те случаи, когда нормализация действительно изменила текст\n",
    "{_.text: _.normal for _ in doc.spans if _.text != _.normal}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6889661d-177e-45cf-a4b0-643faa10260a",
   "metadata": {},
   "source": [
    "## Извлечение именованных сущностей  \n",
    "\n",
    "Разбор именованных сущностей типа PER (личные имена) на компоненты: имя, фамилию и отчество. Зависит от этапа NER (распознавания именованных сущностей). Определяет свойство fact для doc.spans. Внутри использует Yargy-парсер.  \n",
    "\n",
    "В Natasha также встроены инструменты для извлечения дат, денежных сумм и адресов.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63fce8a6-10a1-41e6-87dc-83a8fc1e5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSpan(start=56, stop=63, type='PER', text='Бэггинс', tokens=[...], normal='Бэггинс', fact=DocFact(slots=[...])), DocSpan(start=65, stop=73, type='PER', text='Бэггинсы', tokens=[...], normal='Бэггинсы', fact=DocFact(slots=[...])), DocSpan(start=99, stop=104, type='LOC', text='Холма', tokens=[...], normal='Холм'), DocSpan(start=383, stop=390, type='PER', text='Бэггинс', tokens=[...], normal='Бэггинс', fact=DocFact(slots=[...])), DocSpan(start=465, stop=474, type='LOC', text='Бэггинсов', tokens=[...], normal='Бэггинсов')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Бэггинс': {'first': 'Бэггинс'}, 'Бэггинсы': {'first': 'Бэггинсы'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Итерируемся по всем спанам (span) в документе (doc.spans)\n",
    "# Спан - это выделенный фрагмент текста с определенным типом и границами\n",
    "for span in doc.spans:\n",
    "    # Проверяем, является ли тип текущего спана PER (Person - персона)\n",
    "    if span.type == PER:\n",
    "        # Если это персона, извлекаем факты (например, имя, фамилию) \n",
    "        # с помощью специального экстрактора names_extractor\n",
    "        # Метод extract_fact заполняет атрибут fact у спана\n",
    "        span.extract_fact(names_extractor)\n",
    "\n",
    "# Выводим первые 5 спанов документа для отладки/проверки\n",
    "# Это помогает понять, какие спаны были выделены в тексте\n",
    "print(doc.spans[:5])\n",
    "\n",
    "# Создаем словарь, где ключами будут нормализованные имена (normal),\n",
    "# а значениями - извлеченные факты в виде словаря\n",
    "# Это делаем только для спанов типа PER (персоны)\n",
    "# Синтаксис {_.normal: _.fact.as_dict for _ in doc.spans if _.type == PER} - это генератор словаря:\n",
    "# - _ - это текущий спан в итерации\n",
    "# - if _.type == PER - фильтрация только персон\n",
    "# - _.normal - нормализованное представление имени (например, \"Иван Иванов\" вместо \"Иванов И.\")\n",
    "# - _.fact.as_dict - извлеченные факты о персоне в виде словаря\n",
    "{_.normal: _.fact.as_dict for _ in doc.spans if _.type == PER}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
